{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([          'the',             ',',             '.',            'of',\n",
      "                  'to',           'and',            'in',             'a',\n",
      "                   '\"',            ''s',\n",
      "       ...\n",
      "                'antz',       'monnaie',       'copyist',       'toonami',\n",
      "           'bagration',          'divo',    'convulsive',      'unviable',\n",
      "             'madhava', 'autocephalous'],\n",
      "      dtype='object', name=0, length=100000)\n",
      "hehe <built-in method __sizeof__ of builtin_function_or_method object at 0x0000020C14855800>\n",
      "(2, 50)\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.neighbors\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "word_embeddings = pd.read_csv('C:/Users/bhara/Downloads/glove.6B.50d.txt.zip',\n",
    "                               header=None, sep=' ', index_col=0,\n",
    "                               nrows=100000, compression='zip', encoding='utf-8', quoting=3)\n",
    "print(word_embeddings.index)\n",
    "word_list = word_embeddings.index.values.tolist()\n",
    "word2vec = OrderedDict(zip(word_list, word_embeddings.values))\n",
    "print(\"hehe\", word2vec.values.__sizeof__)\n",
    "# Define a function to compute the average word embedding for a sentence\n",
    "def get_sentence_embedding(sentence):\n",
    "    words = sentence.split()\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in word2vec:\n",
    "            embeddings.append(word2vec[word])\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Define training data and labels\n",
    "sentences = ['This is a positive sentence.', 'This is a negative sentence.']\n",
    "labels = [1, 0]\n",
    "\n",
    "# Compute sentence embeddings for training data\n",
    "X_train = np.array([get_sentence_embedding(sentence) for sentence in sentences])\n",
    "print(X_train.shape)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, labels)\n",
    "\n",
    "# Define test data\n",
    "test_sentence = 'So good, I  cannot accept this'\n",
    "\n",
    "# Compute sentence embedding for test data\n",
    "X_test = np.array([get_sentence_embedding(test_sentence)])\n",
    "\n",
    "# Predict label for test data using logistic regression model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
